---
  - name: Установка Hadoop на кластер
    hosts: all_nodes
    vars:
      ansible_remote_tmp: /tmp
      hadoop_version: 3.4.0
      hadoop_install_dir: /opt/hadoop
      hadoop_user: hadoop
      master_node: "team-42-nn"
      profile_path: "/home/hadoop/.profile"
      java_home: "/usr/lib/jvm/java-11-openjdk-amd64"
      hadoop_home: "/opt/hadoop"

    tasks:
      - name: Размещение /etc/hosts
        become: yes
        copy:
          dest: "/etc/hosts"
          src: node_hosts
          mode: "0644"

      - name: Создание пользователя hadoop
        become: yes
        user:
          name: "{{ hadoop_user }}"
          shell: /bin/bash
          create_home: yes

      - name: Объявление необходимых переменных
        become_user: hadoop
        become: yes
        blockinfile:
          path: "{{ profile_path }}"
          create: yes
          insertafter: EOF
          block: |
            export JAVA_HOME={{ java_home }}
            export HADOOP_HOME={{ hadoop_home }}
            export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$JAVA_HOME/bin

      - name: Установка необходимых пакетов
        become: yes
        apt:
          name:
            - openjdk-11-jdk
            - tar
            - unzip
            - gzip
          state: present
          update_cache: yes

      - name: Загрузка архива Hadoop
        get_url:
          url: "https://dlcdn.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
          dest: /tmp/hadoop-{{ hadoop_version }}.tar.gz
          mode: "755"
          # checksum: sha512:https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz.sha512

      - name: Распаковка архива Hadoop
        unarchive:
          src: /tmp/hadoop-{{ hadoop_version }}.tar.gz
          dest: /tmp/
          remote_src: yes
          creates: /tmp/hadoop-{{ hadoop_version }}

      - name: Копирование в {{ hadoop_install_dir }}
        become: yes
        command: "cp -r /tmp/hadoop-{{ hadoop_version }} {{ hadoop_install_dir }}"
        args:
          creates: "{{ hadoop_install_dir }}"

      - name: Сделать hadoop владельцем {{ hadoop_install_dir }}
        become: yes
        command: chown -R -h hadoop:hadoop {{ hadoop_install_dir }}

      - name: Обновление hadoop-env.sh
        become: yes
        blockinfile:
          path: "{{ hadoop_install_dir }}/etc/hadoop/hadoop-env.sh"
          create: yes
          block: |
            export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
            export HADOOP_HOME={{ hadoop_install_dir }}
            export HADOOP_MAPRED_HOME={{ hadoop_install_dir }}
            export HADOOP_HDFS_HOME={{ hadoop_install_dir }}
            export HADOOP_HOME={{ hadoop_install_dir }}


      - name: Обновление hadoop-env.sh
        become: yes
        blockinfile:
          path: "{{ hadoop_install_dir }}/etc/hadoop/hadoop-env.sh"
          create: yes
          block: |
            export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
            export HADOOP_HOME=/opt/hadoop

      - name: Обновление mapred-env.sh
        become: yes
        blockinfile:
          path: "{{ hadoop_install_dir }}/etc/hadoop/mapred-env.sh"
          create: yes
          block: |
            export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
            export HADOOP_HOME=/opt/hadoop

      - name: Обновление yarn-env.sh
        become: yes
        blockinfile:
          path: "{{ hadoop_install_dir }}/etc/hadoop/yarn-env.sh"
          create: yes
          block: |
            export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
            export HADOOP_HOME=/opt/hadoop

      - name: Конфигурация core-site.xml
        become: yes
        become_user: hadoop
        copy:
          dest: "/opt/hadoop/etc/hadoop/core-site.xml"
          src: core-site.xml.j2

      - name: Конфигурация hdfs-site.xml
        become: yes
        become_user: hadoop
        copy:
          dest: /opt/hadoop/etc/hadoop/hdfs-site.xml
          src: hdfs-site.xml.j2

  - name: Генерация пары SSH ключей для кластера Hadoop
    hosts: localhost
    gather_facts: no
    vars:
      ssh_key_file_path: /tmp/hadoop_ssh_key

    tasks:
      - name: Генерация пары SSH ключей для пользователя hadoop (если не существует)
        ansible.builtin.command:
          cmd: "ssh-keygen -t rsa -b 2048 -f {{ ssh_key_file_path }} -q -N ''"
        args:
          creates: "{{ ssh_key_file_path }}"

      - name: Чтение сгенерированного приватного SSH ключа
        slurp:
          src: "{{ ssh_key_file_path }}"
        register: ssh_private_key_content

      - name: Чтение сгенерированного публичного SSH ключа
        slurp:
          src: "{{ ssh_key_file_path }}.pub"
        register: ssh_public_key_content

      - name: Установка фактов для передачи приватного и публичного ключей
        set_fact:
          hadoop_private_key: "{{ ssh_private_key_content['content'] | b64decode }}"
          hadoop_public_key: "{{ ssh_public_key_content['content'] | b64decode }}"

  - name: Распространение SSH ключей на узлы кластера Hadoop
    hosts: all_nodes
    become: yes
    become_user: hadoop
    tasks:
      - name: Создание директории .ssh для пользователя hadoop
        file:
          path: /home/hadoop/.ssh
          state: directory
          mode: "0700"

      - name: Копирование приватного SSH ключа на каждый узел
        copy:
          content: "{{ hostvars['localhost']['hadoop_private_key'] }}"
          dest: "/home/hadoop/.ssh/id_rsa"
          mode: "0600"

      - name: Копирование публичного SSH ключа на каждый узел
        copy:
          content: "{{ hostvars['localhost']['hadoop_public_key'] }}"
          dest: "/home/hadoop/.ssh/id_rsa.pub"
          mode: "0644"

      - name: Добавление публичного ключа в authorized_keys
        authorized_key:
          user: hadoop
          state: present
          key: "{{ hostvars['localhost']['hadoop_public_key'] }}"
          manage_dir: no

      - name: Создание директории для хранения данных DataNode
        file:
          path: /home/hadoop/hdfs/datanode
          state: directory
          owner: hadoop
          mode: 0755

  - name: Конфигурация Hadoop и YARN
    hosts: all_nodes
    become: yes
    become_user: hadoop
    tasks:
      - name: Упоминание существующих datanodes (etc/hadoop/workers)
        copy:
          dest: /opt/hadoop/etc/hadoop/workers
          content: |
            192.168.1.172
            192.168.1.173

      - name: Конфигурация mapred-site
        copy:
          dest: /opt/hadoop/etc/hadoop/mapred-site.xml
          src: mapred-site.xml.j2

      - name: Конфигурация yarn-site.xml
        copy:
          dest: /opt/hadoop/etc/hadoop/yarn-site.xml
          src: yarn-site.xml.j2

  - name: Запуск Hadoop и YARN
    hosts: master
    become: yes
    become_user: hadoop
    tasks:
      - name: Инициализация хранилища (нобходимо запустить ровно 1 раз, иначе будет вечно выполняться)
        shell: /opt/hadoop/bin/hdfs namenode -format
        ignore_errors: yes
        environment:
          JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64

      # - name: Остановка Hadoop и YARN (если запущены процессы)
      #   shell: /opt/hadoop/sbin/stop-all.sh

      - name: Старт Hadoop-кластера
        shell: /opt/hadoop/sbin/start-dfs.sh

      - name: Старт YARN-кластера
        shell: /opt/hadoop/sbin/start-yarn.sh

      - name: Старт YARN History-сервера
        shell: /opt/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver
